{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_from_scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rhMGZhSKuKqB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self, num_inputs, num_nodes):\n",
        "    # init weights Glorot and Bengio 2010\n",
        "    self.weights = (2*np.random.random((num_inputs, num_nodes)) - 1) * np.sqrt(6.0 / (num_nodes + num_nodes))\n",
        "    self.bias = np.zeros((1, num_nodes))\n",
        "\n",
        "    self.dweights = np.zeros((num_inputs, num_nodes))\n",
        "    self.dbias = np.zeros((1, num_nodes))"
      ],
      "metadata": {
        "id": "GCzFhJNDLTFN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNeuralNetwork:\n",
        "    def __init__(self, input_features, hidden_unit_list, learning_rate = 0.1):\n",
        "        self.n_features = input_features\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.num_layers = len(hidden_unit_list)\n",
        "\n",
        "        self.layers = {}\n",
        "        self.a = {}\n",
        "        self.layers[0] = Layer(self.n_features, hidden_unit_list[0])\n",
        "\n",
        "        for i in range(1, self.num_layers):\n",
        "          self.layers[i] = Layer(hidden_unit_list[i-1], hidden_unit_list[i])\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def stable_softmax(z):\n",
        "        exps = np.exp(z - np.max(z))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "      return np.exp(z)/(1.0+np.exp(z))\n",
        "\n",
        "    def forward_prop(self, X):\n",
        "        # forward prop\n",
        "        self.a[0] = X\n",
        "        for i in range(1, self.num_layers):\n",
        "          z1 = np.dot(self.a[i-1], self.layers[i-1].weights) + self.layers[i-1].bias\n",
        "          # apply nonlinearity (relu)\n",
        "          self.a[i] = np.maximum(0, z1)\n",
        "\n",
        "        z1 = np.dot(self.a[self.num_layers-1], self.layers[self.num_layers-1].weights) + self.layers[self.num_layers-1].bias \n",
        "\n",
        "        # TODO: make below more generalized for different activation functions.         \n",
        "        self.prob = DenseNeuralNetwork.sigmoid(z1)\n",
        "        #self.prob = DenseNeuralNetwork.stable_softmax(z1)\n",
        "        return self.prob\n",
        "\n",
        "    def back_prop(self, X, y):\n",
        "        m = y.shape[0]\n",
        "        dz2 = self.prob\n",
        "        dz2[np.arange(m), y] -= 1\n",
        "        dz2 /= m\n",
        "\n",
        "        for i in reversed(range(self.num_layers)):\n",
        "          self.layers[i].dweights = np.dot(self.a[i].T, dz2)\n",
        "          self.layers[i].dbias = np.sum(dz2, axis=0, keepdims=True)\n",
        "          dz1 = np.dot(dz2, self.layers[i].weights.T)\n",
        "          dz2 = dz1 * (self.a[i] > 0)\n",
        "\n",
        "        return\n",
        "\n",
        "    def update_weights(self):\n",
        "        lr = self.learning_rate\n",
        "        for i in range(1, self.num_layers):\n",
        "          self.layers[i].weights -= lr * self.layers[i].dweights\n",
        "          self.layers[i].bias -= lr * self.layers[i].dbias\n",
        "        return \n",
        "\n",
        "    def compute_loss(self, y):\n",
        "        y = y.astype(int)\n",
        "        m = y.shape[0]\n",
        "        loss = np.sum(-np.log(self.prob[np.arange(m), y]) / m)\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, y, epochs=10000):\n",
        "        for i in range(epochs):\n",
        "            self.forward_prop(X)\n",
        "\n",
        "            # log loss along the way\n",
        "            if i % 2000 == 0:\n",
        "              loss = self.compute_loss(y)\n",
        "              print(f\"iteration: {i} loss: {loss}\")\n",
        "\n",
        "            self.back_prop(X, y)\n",
        "            self.update_weights()\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "KhC3GdYBnmV3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test using Generated Data (same data as what we used in Logistic Regression implementation)"
      ],
      "metadata": {
        "id": "IxJdAS33KsAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "(X, y) = datasets.make_classification(n_samples=10000, n_features=7, n_informative=5, n_redundant=2)\n",
        "\n",
        "#Split in test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "cDbMUWn3mUh0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = DenseNeuralNetwork(7, [25, len(np.unique(y))])\n",
        "nn.train(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s19qlbAamUfO",
        "outputId": "9d4f7e48-9e5b-4871-9133-6a04c39f2734"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 25\n",
            "25 2\n",
            "iteration: 0 loss: 1.1548861210939627\n",
            "iteration: 2000 loss: 0.3349812553188528\n",
            "iteration: 4000 loss: 0.3070030530101862\n",
            "iteration: 6000 loss: 0.2953476722118449\n",
            "iteration: 8000 loss: 0.28892488545762374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nn.forward_prop(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "s_emMUPxmUc3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {np.sum(y_test==y_pred)/X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulFmYTx1mUaR",
        "outputId": "c1c19770-5b8a-4116-cd50-e08dca364556"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8845\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Network_from_scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rhMGZhSKuKqB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self, num_inputs, num_nodes):\n",
        "    # init weights Glorot and Bengio 2010\n",
        "    self.weights = (2*np.random.random((num_inputs, num_nodes)) - 1) * np.sqrt(6.0 / (num_nodes + num_nodes))\n",
        "    self.bias = np.zeros((1, num_nodes))\n",
        "\n",
        "    # Kaiming weights\n",
        "    self.dweights = np.zeros((num_inputs, num_nodes))\n",
        "    self.dbias = np.zeros((1, num_nodes))"
      ],
      "metadata": {
        "id": "GCzFhJNDLTFN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, X, y, num_hidden_units = 25, l1_lambda = 1e-3, learning_rate = 0.1):\n",
        "        # m for #training examples and n for #features\n",
        "        self.n_obs, self.n_features = X.shape\n",
        "\n",
        "        # regularization term lambda_ (lambda is reserved keyword)\n",
        "        self.lambda_ = l1_lambda\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        self.l1 = Layer(self.n_features, num_hidden_units)\n",
        "        self.l2 = Layer(num_hidden_units, len(np.unique(y)))\n",
        "\n",
        "        self.a0 = X\n",
        "        self.a1 = np.zeros((self.n_obs, num_hidden_units))\n",
        "        self.a2 = np.zeros((self.n_obs, len(np.unique(y))))\n",
        "\n",
        "    @staticmethod\n",
        "    def stable_softmax(z):\n",
        "        exps = np.exp(z - np.max(z))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    def forward_prop(self, X):\n",
        "        W2 = self.l2.weights\n",
        "        W1 = self.l1.weights\n",
        "        b2 = self.l2.bias\n",
        "        b1 = self.l1.bias\n",
        "\n",
        "        # forward prop\n",
        "        a0 = X\n",
        "        z1 = np.dot(a0, W1) + b1\n",
        "\n",
        "        # apply nonlinearity (relu)\n",
        "        a1 = np.maximum(0, z1)\n",
        "        z2 = np.dot(a1, W2) + b2\n",
        "\n",
        "        probs = NeuralNetwork.stable_softmax(z2)\n",
        "\n",
        "        self.a0 = X\n",
        "        self.probs = probs\n",
        "        self.a1 = a1\n",
        "        self.a2 = probs\n",
        "        return probs\n",
        "\n",
        "    def back_prop(self):\n",
        "        W2 = self.l2.weights\n",
        "        W1 = self.l1.weights\n",
        "        b2 = self.l2.bias\n",
        "        b1 = self.l1.bias\n",
        "\n",
        "        dz2 = self.probs\n",
        "        dz2[np.arange(self.n_obs), self.y] -= 1\n",
        "        dz2 /= self.n_obs\n",
        "\n",
        "        self.l2.dweights = np.dot(self.a1.T, dz2) + self.lambda_ * W2\n",
        "        self.l1.dbias = np.sum(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        dz1 = np.dot(dz2, W2.T)\n",
        "        dz1 = dz1 * (self.a1 > 0)\n",
        "\n",
        "        self.l1.dweights = np.dot(self.a0.T, dz1) + self.lambda_ * W1\n",
        "        self.l1.dbias = np.sum(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        return\n",
        "\n",
        "    def update_weights(self):\n",
        "        lr = self.learning_rate\n",
        "\n",
        "        # take a step along negative gradient\n",
        "        self.l2.weights -= lr * self.l2.dweights\n",
        "        self.l1.weights -= lr * self.l1.dweights\n",
        "        self.l2.bias -= lr * self.l2.dbias\n",
        "        self.l1.bias -= lr * self.l1.dbias\n",
        "\n",
        "        return \n",
        "\n",
        "    def compute_loss(self):\n",
        "        W2 = self.l2.weights\n",
        "        W1 = self.l1.weights\n",
        "\n",
        "        y = self.y.astype(int)\n",
        "        data_loss = np.sum(-np.log(self.probs[np.arange(self.n_obs), self.y]) / self.n_obs)\n",
        "        l2_reg_loss = 0.5 * self.lambda_ * np.sum(W1 * W1) + 0.5 * self.lambda_ * np.sum(W2 * W2)\n",
        "        loss = data_loss + l2_reg_loss\n",
        "        return loss\n",
        "\n",
        "    def train(self, X, y, epochs=10000):\n",
        "        for i in range(epochs):\n",
        "            self.forward_prop(X)\n",
        "\n",
        "            # log loss along the way\n",
        "            if i % 2000 == 0:\n",
        "              loss = self.compute_loss()\n",
        "              print(f\"iteration: {i} loss: {loss}\")\n",
        "\n",
        "            self.back_prop()\n",
        "            self.update_weights()\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "KhC3GdYBnmV3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test using Generated Data (same data as what we used in Logistic Regression implementation)"
      ],
      "metadata": {
        "id": "IxJdAS33KsAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "(X, y) = datasets.make_classification(n_samples=10000, n_features=7, n_informative=5, n_redundant=2)\n",
        "\n",
        "#Split in test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "cDbMUWn3mUh0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(X_train, y_train)\n",
        "nn.train(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s19qlbAamUfO",
        "outputId": "8c362f25-4e64-4c21-fb86-6af44c28edee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 0 loss: 1.2408950696948744\n",
            "iteration: 2000 loss: 0.13631904988868987\n",
            "iteration: 4000 loss: 0.1309320965348822\n",
            "iteration: 6000 loss: 0.1277427847090521\n",
            "iteration: 8000 loss: 0.12600546267699875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = nn.forward_prop(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "s_emMUPxmUc3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {np.sum(y_test==y_pred)/X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulFmYTx1mUaR",
        "outputId": "0e20e02c-086f-4145-82b2-efda1abd1f6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.969\n"
          ]
        }
      ]
    }
  ]
}

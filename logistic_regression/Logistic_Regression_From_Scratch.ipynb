{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression_From_Scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression implementation from scratch"
      ],
      "metadata": {
        "id": "5ba7wI2P1BH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "  def __init__(self, X, y, learning_rate, max_iterations, tolerance):\n",
        "    self.X = X\n",
        "    self.y = np.expand_dims(y, axis=1)       \n",
        "    self.beta = np.random.rand(self.X.shape[1], 1) / 10.0\n",
        "    self.bias = 0\n",
        "    self.lr = learning_rate\n",
        "    self.max_iter = max_iterations\n",
        "    self.n_obs = X.shape[0]\n",
        "    self.n_features = X.shape[1]\n",
        "    self.tol = tolerance\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))      \n",
        "\n",
        "  def yhat(self, beta, bias):\n",
        "    yhat = self.sigmoid(np.matmul(self.X, beta) + bias)\n",
        "    return yhat\n",
        "\n",
        "  def loss(self, beta, bias):\n",
        "    yhat = self.yhat(beta, bias)\n",
        "    loss = -1.0 / self.n_obs * (np.sum(self.y * np.log(yhat) + (1.0 - y) * np.log(1.0 - yhat)))\n",
        "    return loss\n",
        "\n",
        "  def gradient_descent(self, beta, bias):\n",
        "    yhat = self.yhat(beta, bias)\n",
        "    dLossdBeta = 1.0 / self.n_obs * np.matmul(self.X.T, (yhat - self.y))\n",
        "    dLossdBias = (1.0 / self.n_obs * np.sum(yhat - self.y))\n",
        "    return dLossdBeta, dLossdBias\n",
        "\n",
        "  def train(self):\n",
        "    for i in range(self.max_iter):\n",
        "      beta_old = self.beta\n",
        "      bias_old = self.bias\n",
        "      dLossdBeta, dLossdBias = self.gradient_descent(self.beta, self.bias)\n",
        "      self.beta = self.beta - self.lr * dLossdBeta\n",
        "      self.bias = self.bias - self.lr * dLossdBias\n",
        "      diff = abs(self.beta - beta_old)\n",
        "      diff_over_threshold = diff > self.tol * self.beta\n",
        "      if not diff_over_threshold.any():\n",
        "        loss = self.loss(self.beta, self.bias)\n",
        "        print(f\"Converged!!! Final loss: {loss}\")\n",
        "        break  \n",
        "      if i % 10000 == 0:\n",
        "        loss = self.loss(self.beta, self.bias)\n",
        "        print(f\"Loss at iteration {i} is {loss}\")\n",
        "    print(\"Bias = \", self.bias)\n",
        "    print(\"Beta = \", np.squeeze(self.beta, axis=1))\n",
        "\n",
        "  def predict(self, X):\n",
        "    yhat = np.round_(self.sigmoid(np.matmul(X, self.beta) + self.bias))\n",
        "    return yhat"
      ],
      "metadata": {
        "id": "kxNFyQve3K2E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test1 using Generated Data"
      ],
      "metadata": {
        "id": "dRi-qAQ8n6w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs"
      ],
      "metadata": {
        "id": "2Z3VbzZCrvI0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_blobs(n_samples=5000, centers=2)\n",
        "\n",
        "#Split in test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "KjVJI3q3rvBh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(X_train, y_train, 0.001, 100000, 1e-4)"
      ],
      "metadata": {
        "id": "lmZXlDXiru67"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0WoSbLasNJh",
        "outputId": "ec964fdf-f032-400a-b8a9-31aa2cea5edf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at iteration 0 is 4017.409934738068\n",
            "Loss at iteration 10000 is 9987.996560420366\n",
            "Loss at iteration 20000 is 11016.663977314098\n",
            "Loss at iteration 30000 is 11620.480629522288\n",
            "Loss at iteration 40000 is 12049.967413850016\n",
            "Loss at iteration 50000 is 12383.7666260662\n",
            "Loss at iteration 60000 is 12656.954344243035\n",
            "Loss at iteration 70000 is 12888.262601000411\n",
            "Loss at iteration 80000 is 13088.884038388913\n",
            "Loss at iteration 90000 is 13266.044694403141\n",
            "Bias =  -0.07362037909603185\n",
            "Beta =  [ 0.48200607 -0.95192122]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "y_test1 = np.expand_dims(y_test, axis=1)  \n",
        "\n",
        "print(f\"Accuracy: {np.sum(y_test1==y_pred)/X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiIFBLjIsNF6",
        "outputId": "d931c545-44b7-46f8-df24-4462b2693143"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test 2 "
      ],
      "metadata": {
        "id": "aVb1lebUtcq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a fake dataset\n",
        "from sklearn import datasets\n",
        "(X, y) = datasets.make_classification(n_samples=10000, n_features=7, n_informative=5, n_redundant=2)"
      ],
      "metadata": {
        "id": "zVTVf0kZn3p_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split in test train split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "vRPqcQSWn3iS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(X_train, y_train, 0.00001, 50000, 1e-4)"
      ],
      "metadata": {
        "id": "EcWnFUOJoYE5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SviLcXeowzg",
        "outputId": "a2dc5288-a9c0-4da1-82a5-a324b23b9184"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at iteration 0 is 6742.95793669236\n",
            "Loss at iteration 10000 is 6403.216906726441\n",
            "Loss at iteration 20000 is 6408.2228026918665\n",
            "Loss at iteration 30000 is 6515.635248604295\n",
            "Loss at iteration 40000 is 6651.9667605796385\n",
            "Bias =  -0.007143715517786395\n",
            "Beta =  [ 0.10946463  0.16638967 -0.0250479  -0.05617942 -0.09759045  0.31229916\n",
            "  0.04734305]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "y_test1 = np.expand_dims(y_test, axis=1)  \n",
        "\n",
        "print(f\"Accuracy: {np.sum(y_test1==y_pred)/X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6sreLpirH1l",
        "outputId": "f5f89893-b0bf-4678-8ada-477289e7361f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8675\n"
          ]
        }
      ]
    }
  ]
}